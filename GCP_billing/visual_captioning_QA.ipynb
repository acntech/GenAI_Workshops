{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uxCkB_DXTHzf"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hny4I-ODTIS6"
   },
   "source": [
    "# Visual captioning with Imagen on Vertex AI\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-nLS57E2TO5y"
   },
   "source": [
    "## Overview\n",
    "\n",
    "[Imagen on Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/image/overview) (image Generative AI) offers a variety of features:\n",
    "- Image generation\n",
    "- Image editing\n",
    "- Visual captioning\n",
    "- Visual question answering\n",
    "\n",
    "This notebook focuses on **visual captioning** only.\n",
    "\n",
    "[Visual captioning with Imagen on Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/image/image-captioning) can generate text descriptions of images. The model takes in an image as input and produces one or more text descriptions of the image as output. The generated text descriptions can be used for a variety of use cases:\n",
    "- getting detailed metadata about images for storing and searching\n",
    "- generating automated captioning to support accessibility use cases\n",
    "- producing descriptions of products and visual assets\n",
    "\n",
    "More information about Visual captioning with Imagen on  Vertex AI can be found in the [official documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/image/image-captioning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iXsvgIuwTPZw"
   },
   "source": [
    "### Objectives\n",
    "\n",
    "In this notebook, you will learn how to use the Vertex AI Python SDK to:\n",
    "\n",
    "- Generate image captions using the Imagen's visual captioning features\n",
    "\n",
    "- Experiment with different parameters, such as:\n",
    "    - number of captions to be generated\n",
    "    - language of the generated captions\n",
    "    - type and version of model that is used to generate the captions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skXAu__iqks_"
   },
   "source": [
    "### Costs\n",
    "\n",
    "- This notebook uses billable components of Google Cloud:\n",
    "  - Vertex AI (Imagen)\n",
    "\n",
    "- Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvKl-BtQTRiQ"
   },
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwFMpIMrTV_4"
   },
   "source": [
    "### Install Vertex AI SDK, other packages and their dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WYUu8VMdJs3V",
    "outputId": "e7ca7962-edde-4023-bfeb-3fdfbdda5cab"
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --user google-cloud-aiplatform>=1.29.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5Xep4W9lq-Z"
   },
   "source": [
    "### Restart current runtime\n",
    "\n",
    "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XRvKdaPDTznN",
    "outputId": "77e030b7-896f-49df-b0ed-5f57a5e68f04"
   },
   "outputs": [],
   "source": [
    "# Restart kernel after installs so that your environment can access the new packages\n",
    "import IPython\n",
    "import time\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbmM4z7FOBpM"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opUxT_k5TdgP"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "Since you are running this notebook on Google Colab, you will need to authenticate your environment. To do this, run the new cell below. These steps are not required if you are using [Vertex AI Workbench](https://cloud.google.com/vertex-ai-workbench)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **IMPORTANT:**\n",
    "If you are running this during the Accenture workshop 24 August 2023, we have decided to give you a billing account to use. Upload the json key sent to you on teams after running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run this cell and ADD THE JSON FILE sent to you on Teams by pressing the \"Choose Files\" button\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))\n",
    "  \n",
    "# Then move the uploaded file to the desired location and set the environment variable\n",
    "!mv {fn} '/content/key.json'\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/content/key.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did not receive the json key for this workshop you'll need to activate your own GCP billing account (enabling Vertex AI) and authenticate by uncommenting the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vbNgv4q1T2Mi"
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# # Addtional authentication is required for Google Colab\n",
    "# if 'google.colab' in sys.modules:\n",
    "\n",
    "#     # Authenticate user to Google Cloud\n",
    "#     from google.colab import auth\n",
    "#     auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybBXSukZkgjg"
   },
   "source": [
    "### Define Google Cloud project information (Colab only)\n",
    "\n",
    "Since you are running this notebook on Google Colab, you need to define Google Cloud project information to be used. In the following cell, you will define the information, import Vertex AI package, and initialize it. This step is not required if you are using [Vertex AI Workbench](https://cloud.google.com/vertex-ai-workbench)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5gUjJ42Nh5kf"
   },
   "outputs": [],
   "source": [
    "if 'google.colab' in sys.modules:\n",
    "    \n",
    "    # Define project information\n",
    "    PROJECT_ID = \"your-project-id\" # @param {type:\"string\"}\n",
    "    LOCATION = \"us-central1\" # @param {type:\"string\"}\n",
    "\n",
    "    # Initialize Vertex AI\n",
    "    import vertexai\n",
    "    vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhfneknwEDHT"
   },
   "source": [
    "### Load the image captioning model\n",
    "\n",
    "The model names from Vertex AI Imagen have two components: model name and version number. The naming convention follow this format: `<model-name>@<version-number>`. For example, `imagetext@001` represent the version **001** of the **imagetext** model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VuEbYyfM4RR7"
   },
   "outputs": [],
   "source": [
    "from vertexai.preview.vision_models import ImageCaptioningModel\n",
    "\n",
    "image_captioning_model = ImageCaptioningModel.from_pretrained(\"imagetext@001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zoPQnQoED10"
   },
   "source": [
    "### Load the image file\n",
    "\n",
    "To use the visual captioning model, you first need to create an `Image` class using the image file. The model only accepts `Image` class objects, so this is a necessary step before you can generate captions.\n",
    "\n",
    "Moreover, [Visual Captioning with Imagen](https://cloud.google.com/vertex-ai/docs/generative-ai/image/image-captioning) only accepts specific image file formats (e.g. PNG, JPEG), and may have file size is limitations (e.g. 10 MB). You can find out specific details from [this official documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/image/image-captioning#img-cap-rest).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S00UD6_aldHy",
    "outputId": "18252c1c-7c89-455e-9e55-6e17ce60a340"
   },
   "outputs": [],
   "source": [
    "# Downloading an image from Google Cloud Storage\n",
    "\n",
    "! gsutil cp \"gs://cloud-samples-data/vision/using_curl/shanghai.jpeg\" ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "id": "xnH-MARxgGrX",
    "outputId": "9db52521-b56d-4ed9-ba3d-abb3f1701d4d"
   },
   "outputs": [],
   "source": [
    "from vertexai.preview.vision_models import Image\n",
    "\n",
    "# Load the image file as Image object\n",
    "shanghai_image = Image.load_from_file(\"shanghai.jpeg\")\n",
    "shanghai_image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYjS3nL5LTbY"
   },
   "source": [
    "###  Generate captions from the image\n",
    "\n",
    "In this section, you will use the visual captioning model to generate text descriptions of an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FeBCOavpRl4y",
    "outputId": "12353348-35f1-4a68-eaca-fdac91c9402b"
   },
   "outputs": [],
   "source": [
    "# Get a caption from the image\n",
    "image_captioning_model.get_captions(\n",
    "    image=shanghai_image,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oJ-WiCEUZQp"
   },
   "source": [
    "### Generating captions in non-English languages\n",
    "\n",
    "Visual captioning with Imagen on Vertex AI can generate captions in multiple languages as well. To generate a caption in a specific language, you can set the `language` parameter as one of the values:\n",
    "- `no` - Norwegian\n",
    "- `en` - English\n",
    "- `fr` - French\n",
    "- `de` - German\n",
    "- `it` - Italian\n",
    "- `es` - Spanish\n",
    "\n",
    "For a list of supported languages, check out the [official documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/image/image-captioning#languages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TilOotKGk9MF",
    "outputId": "fbaa921a-b169-445a-c24c-8462dce19e79"
   },
   "outputs": [],
   "source": [
    "# Get 3 image captions in French\n",
    "image_captioning_model.get_captions(\n",
    "    image=shanghai_image,\n",
    "    number_of_results=3,\n",
    "    language=\"no\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can generate up to three separate captions from a single image by changing the `number_of_results` parameter from 1 to 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CsB8pSS-h7Sw"
   },
   "source": [
    "## Try it yourself\n",
    "\n",
    "You can also try using the visual captioning model with images of your choice. If you need to download an image file, you can use the provided auxiliary function `download_image`.\n",
    "\n",
    "Feel free to experiment with different images and model parameters to see how the results change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "KgPcf_SEBKKW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "def download_image(url):\n",
    "    \"\"\"Downloads an image from the specified URL.\"\"\"\n",
    "\n",
    "    # Send a get request to the url\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # If the request is successful\n",
    "    if response.status_code == 200:\n",
    "\n",
    "        # Define image related variables\n",
    "        image_path = os.path.basename(url)\n",
    "        image_bytes = response.content\n",
    "        image_type = response.headers['Content-Type'].split('/')[1]\n",
    "\n",
    "        # Check for image type, currently only PNG or JPEG format are supported\n",
    "        if image_type in (\"png\", \"jpg\", \"jpeg\"):\n",
    "\n",
    "            # Write image data to a file\n",
    "            with open(image_path, \"wb\") as f:\n",
    "                f.write(image_bytes)\n",
    "            return image_path\n",
    "        else:\n",
    "            raise Exception(\"Image can only be in PNG or JPEG format\")\n",
    "\n",
    "    else:\n",
    "        raise Exception(f\"Failed to download image from {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "bm5Nt9Yj6N88"
   },
   "outputs": [],
   "source": [
    "# Download an image\n",
    "url = \"https://storage.googleapis.com/gweb-cloudblog-publish/images/transfor_shared_fate.0999075519991154.max-2000x2000.jpg\"\n",
    "image_path = download_image(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 440
    },
    "id": "eBN2j384jPbm",
    "outputId": "d0b3c0d0-89d5-45e7-8637-410f0bcd0096"
   },
   "outputs": [],
   "source": [
    "# Load the newly downloaded image\n",
    "user_image = Image.load_from_file(image_path)\n",
    "user_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ilm66OqkVneT",
    "outputId": "13abddb1-913f-42bb-df43-f2b4226b8faf"
   },
   "outputs": [],
   "source": [
    "# Generate the visual captions for the image in Norwegian:\n",
    "image_captioning_model.get_captions(\n",
    "    image=user_image,\n",
    "    number_of_results=3,\n",
    "    language='en',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask questions about the image\n",
    "\n",
    "Now ask questions about the image using the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the image question answering model\n",
    "\n",
    "The model names from Vertex AI Imagen have two components: model name and version number. The naming convention follow this format: `<model-name>@<version-number>`. For example, `imagetext@001` represents the version **001** of the **imagetext** model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.preview.vision_models import ImageQnAModel\n",
    "\n",
    "image_qna_model = ImageQnAModel.from_pretrained(\"imagetext@001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question about the image\n",
    "image_qna_model.ask_question(\n",
    "  image=shanghai_image,\n",
    "  question=\"What is happening in this image?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a follow up question about the image\n",
    "image_qna_model.ask_question(\n",
    "    image=shanghai_image,\n",
    "    question=\"What are the people in the image doing?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get up to three answers from a single image by changing the `number_of_results` parameter from 1 to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 3 answers from the image\n",
    "image_qna_model.ask_question(\n",
    "    image=shanghai_image,\n",
    "    question=\"What are the people in the image doing?\",\n",
    "    number_of_results=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it yourself\n",
    "\n",
    "You can also try using the visual question answering model with images of your choice. If you need to download an image file, you can use the provided auxiliary function `download_image`.\n",
    "\n",
    "Feel free to experiment with different images and model parameters to see how the results change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download an image\n",
    "url = \"https://storage.googleapis.com/gweb-cloudblog-publish/images/GettyImages-871168786.max-2600x2600.jpg\"\n",
    "image_path = download_image(url)\n",
    "\n",
    "# Load the newly downloaded image\n",
    "user_image = Image.load_from_file(image_path)\n",
    "user_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question about the image\n",
    "image_qna_model.ask_question(\n",
    "    image=user_image,\n",
    "    question=\"What is happening in this photo?\",\n",
    "    number_of_results=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question about the image\n",
    "image_qna_model.ask_question(\n",
    "    image=user_image,\n",
    "    question=\"What advertising channels would this image be suitable for?\",\n",
    "    number_of_results=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question about the image\n",
    "image_qna_model.ask_question(\n",
    "      image=user_image,\n",
    "      question=\"What type of insects could live in this area?\",\n",
    "      number_of_results=3,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m110",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m110"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
